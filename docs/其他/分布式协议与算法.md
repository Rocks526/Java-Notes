# 一：分布式理论基础

### <a id="bzt">拜占庭将军问题</a>

> 拜占庭将军问题是借一个故事展示分布式共识的问题，同时讨论了分布式达成共识的解决方法。

- 故事背景

在很久很久以前，拜占庭是东罗马帝国的首都。那个时候罗马帝国国土辽阔，为了防御目的，因此每个军队都分隔很远，将军与将军之间只能靠信使传递消息。

在打仗的时候，拜占庭军队内所有将军必需达成**一致的共识**，才能更好地赢得胜利。但是，在军队内有可能存有叛徒，扰乱将军们的决定，并且信使传递过程中也可能被拦截甚至被间谍替换。

这时候，在已知有成员不可靠的情况下，其余忠诚的将军需要在不受叛徒或间谍的影响下达成一致的协议。

莱斯利·兰伯特（ Leslie Lamport ）通过这个比喻，表达了计算机网络中所存在的一致性问题。这个问题被称为**拜占庭将军问题**。

- 两忠一叛难题

三位将军商量最终作战方案，其中两位忠诚，一位是叛徒。

- 解决方案

1. **口信消息型拜占庭将军问题之解**

首先所有将军都设置没有接到命令时的默认命令，然后随机一个将军向所有将军发送要执行的命令，之后其他所有将军将自己收到的命令发送给别的将军，最终所有将军执行收到的数量多的命令。

> 达成一致的前提条件：如果叛徒的数量是M，将军人数不能少于3M+1，并且需要M轮协商。
>
> 通过增加忠诚将军的数量实现最终一致性。

2. **签名消息型拜占庭问题之解**

还可以通过签名的方式，在不增加将军人数的情况下，解决二忠一叛的难题。签名需要具备以下特点：

忠诚将军的签名无法伪造，而且对他签名消息的内容进行任何更改都会被发现；
任何人都能验证将军签名的真伪。

当忠诚将军发现签名被伪造时，即知道该消息是叛徒发送，则执行另外忠诚将军发送的命令。

- 总结

> 将军 ==>  计算机节点
>
> 忠诚将军  ==>  运行正常的计算机节点
>
> 叛变将军  ==>  出现故障并且会发送误导信息的节点
>
> 信使被杀  ==>   通信故障，消息丢失
>
> 信使被间谍替换  ==>  通信被中间人攻击，信息被伪造

拜占庭将军问题描述的是困难的，也是复杂的一种分布式故障场景，除了存在故障行为，还存在恶意行为的一个场景。

在存在恶意节点行为的场景中（比如在数字货币的区块链技术中），必须使用拜占庭容错算法（Byzantine Fault Tolerance，BFT）。除了上面提到两种算法，常用的拜占庭容错算法还有：**PBFT算法，PoW算法**。

而在计算机分布式系统中，常用的是非拜占庭容错算法，即故障容错算法（Crash Fault Tolerance，CFT）。CFT 解决的是分布式的系统中存在故障，但不存在恶意节点的场景下的共识问题。 也就是说，这个场景可能会丢失消息，或者有消息重复，但不存在错误消息，或者伪造消息的情况。常见的算法有**Paxos算法、Raft算法、ZAB协议。**

### <a id="cap">CAP理论</a>

CAP理论是对分布式系统的特性做出的抽象，总结出一致性、可用性和分区容错性三个特性。**而在分布式系统中，这三个特性不可兼得，只能选择其中两个。**

- 一致性（Consistency）

一致性说的是客户端的每次读操作，不管访问哪个节点，要么读到的都是同一份最新的数据，要么读取失败。

> 一致性强调的是各节点的数据一致性，而不是数据完整性。

- 可用性（Availability）

可用性说的是任何来自客户端的请求，不管访问哪个节点，都能得到响应数据，但不保证是同一份最新数据。

也可以把可用性看作是分布式系统对访问本系统的客户端的另外一种承诺：我尽力给你返回数据，不会不响应你，但是我不保证每个节点给你的数据都是最新的。 

这个指标强调的是服务可用，但不保证数据的一致。

- 分区容错性（Partition Tolerance）

分区容错性说的是当节点间出现任意数量的消息丢失或高延迟的时候，系统仍然可以继续提供服务。

也就是说，分布式系统在告诉访问本系统的客户端：不管我的内部出现什么样的数据同步问题，我会一直运行，提供服务。

这个指标，强调的是集群对分区故障的容错能力。

> 因为分布式系统与单机系统不同，它涉及到多节点间的通讯和交互，节点间的分区故障是必然发生的，因此分区容错性必须满足，一般在可用性和数据一致性之间做出选择。

CP：当选择了一致性（C）的时候，如果因为消息丢失、延迟过高发生了网络分区，部分节点无法保证特定信息是最新的，那么这个时候，当集群节点接收到来自客户端的写请求时，因为无法保证所有节点都是最新信息，所以系统将返回写失败错误，也就是说集群拒绝新数据写入。
AP：当选择了可用性（A）的时候，系统将始终处理客户端的查询，返回特定信息，如果发生 了网络分区，一些节点将无法返回最新的特定信息，它们将返回自己当前的相对新的信息。

> 并非在任何情况下，分布式系统都只能在AC中选择一个，当系统正常运行时(没有产生网络分区时)，P是不需要被满足的，此时是可以同时保证AC的。只有当系统网络出现分区，必须满足P时，AC只能选择一个。
>

- 总结

CA 模型：在分布式系统中不存在。因为舍弃 P，意味着舍弃分布式系统，就比如单机版关系型数据库MySQL，如果MySQL要考虑主备或集群部署时，它必须考虑P。

CP 模型：采用CP模型的分布式系统，一旦因为消息丢失、延迟过高发生了网络分区， 就影响用户的体验和业务的可用性。因为为了防止数据不一致，集群将拒绝新数据的写入，典型的应用是ZooKeeper，MongoDB，Etcd 和 HBase。

AP 模型：采用 AP 模型的分布式系统，实现了服务的高可用。用户访问系统的时候，都能得到响应数据，不会出现响应错误，但当出现分区故障时，相同的读操作，访问不同的节点，得到响应数据可能不一样。典型应用就比如Cassandra 和DynamoDB。

### <a id="acid">ACID理论</a>

事务是指访问并可能更新数据库中各种数据项的一个程序执行单元(unit)，通过事务，可以保证数据库的数据总是处于一个一致性的状态下。事务具备ACID特性：

- 原子性（atomicity)

一个事务要么全部提交成功，要么全部失败回滚，不能只执行其中的一部分操作

- 一致性（consistency)

事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行之前和执行之后，数据库都必须处于一致性状态。

> 如果数据库系统在运行过程中发生故障，有些事务尚未完成就被迫中断，这些未完成的事务对数据库所作的修改有一部分已写入物理数据库，这是数据库就处于一种不正确的状态，也就是不一致的状态。
>
> 此时事务会通过回滚让数据库重新回到一致性状态。

- 隔离性（isolation）

事务的隔离性是指在并发环境中，并发的事务是相互隔离的，一个事务的执行不能被其他事务干扰。

不同的事务并发操作相同的数据时，每个事务都有各自完成的数据空间，即一个事务内部的操作及使用的数据对其他并发事务是隔离的，并发执行的各个事务之间不能相互干扰。

> 在标准SQL规范中，定义了4个事务隔离级别，不同的隔离级别对事务的处理不同，分别是：
>
> - 读未提交（Read Uncommited）：当多个事务并发时，某个事务可以读取其他事务修改后的数据，即便该事务还没有提交
> - 已提交读（Read Commited）：当多个事务并发时，某个事务只能读取其他事务提交后的数据
> - 可重复读（Repeatable Read)：保证在事务处理过程中，多次读取同一个数据时，其值都和事务开始时刻是一致的(解决某个事务执行过程中，别的事务提交了修改数据，导致该事务两次读取不一致)
> - 串行化：最严格的事务隔离级别，它要求所有事务被串行执行，即事务只能一个接一个的进行处理，不能并发执行。
>
> 不同数据库对RR级别实现不一致，导致有的可以解决幻读，有的不能解决幻读。

- 持久化（durability）

一旦事务提交，那么它对数据库中的对应数据的状态的变更就会永久保存到数据库中。--即使发生系统崩溃或机器宕机等故障，只要数据库能够重新启动，那么一定能够将其恢复到事务成功结束的状态。

> 可以认为，实现了事务，就可以保证数据库安全，即数据一致性。在单机可以通过加锁，时间序列，MVCC等机制实现事务，在分布式环境，想要实现事务需要通过分布式事务协议，如二阶段提交协议，三阶段提交协议和TCC协议。

-------------------------

- 二阶段提交协议

分布式事务的过程是将事务发送到多个节点，多个节点执行属于自己的一部分，如果有任意一个节点执行失败，则整个事务得回滚，涉及多个节点，而且需要一个角色收集所有节点的处理情况，实现繁琐。

因此出现了二阶段提交协议，二阶段协议提出了协调者的概念，由协调者负责收集各个节点的处理情况，向客户端返回响应。

除此以外，在正式执行事务之前，还加入一个准备阶段，将事务发送给各个节点，各节点判断是否可执行，如果可执行，则锁定资源并返回协调者yes，协调者收到所有yes再让各节点提交事务，如果有一个节点不是yes，则通知各节点不执行事务，释放锁定的资源。

> 二阶段提交协议整体上只是一种思想，核心在于两点：
>
> - 通过引入协调者，实现各节点处理状态的收集
> - 在正式执行之前加入一个准备阶段，统计事务能否进行执行
>
> 二阶段提交协议如果要在工程上实现，还有很多细节需要补充，如协调者决定提交事务后，发送给各节点提交事物的消息丢失，或者有节点宕机如何处理。
>
> 在二阶段提交协议中，只是规定各节点只要返回协调者yes，即便之后消息丢失或者宕机也要能提交事务，具体如何实现，有不同的方式。
>
> 参考：https://blog.csdn.net/lengxiao1993/article/details/88290514

二阶段提交协议最早是用来实现数据库的分布式事务的，不过现在最常用的协议是XA协议。这个协议是 X/Open国际联盟基于二阶段提交协议提出的，也叫作X/Open Distributed Transaction Processing（DTP）模型，比如MySQL就是通过MySQL XA实现了分布式事务。

无论是二阶段提交协议还是XA协议，都存在一些问题：

1. 协调者存在单点故障问题
2. 在准备阶段需要预留资源，将事务涉及的相关资源锁定，当协调者挂掉，一直不发送commit时，执行者将资源一直锁定
3. 事务状态丢失的问题，即便协调者做双机热备，挂掉自动选举出新的协调者，当协调者和某个执行者同时挂掉的时候，新的协调者无法得知挂掉的执行者是佛偶执行了commit
4. 脑裂问题：当由于网络原因，部分执行者没有收到commit消息，而其他执行者收到了commit消息，就会导致数据不一致

--------------------------------

- 三阶段提交协议

为了缓解和优化两阶段提交协议存在的问题，出现了三阶段提交协议。将整个分布式事务分为canCommit，preCommit，doCommit三个阶段进行，同时引入超时机制和默认操作机制。

canCommit阶段：检查各个执行者状态，是否可以执行事务

preCommit阶段：预留资源，将事务涉及的资源进行锁定，类似于两阶段提交的准备阶段

doCommit阶段：通知各个执行者执行事务

> 为了解决单点问题，引入超时机制和默认操作，当协调者挂掉时，执行者长期等不到commit执行，会自行执行默认的commit，同时解决资源长期占用问题。
>
> 引入canCommit阶段的目的在于，只有canCommit全部返回yes，才能发送preCommit指令，当协调者超时后，大家都默认commit，在两阶段提交协议里，只有一个准备阶段，执行者无法得知其他执行者是否可执行。
>
> 三阶段提交同样存在脑裂问题，当协调者在doCommit阶段，不发送commit指令，而是发送回滚指令，如果某个执行者出现脑裂，会无法收到消息，执行commit导致数据不一致
>
> 无论是两阶段提交还是三阶段提交，一般都适合在单机操作多个数据库的场景，而在现在的微服务架构下，基本是每个服务对应自己的库，操作别的库需要调用别的服务接口，一般不存在这种场景，因此使用很少，基本是在数据库层面实现的。

-------------------------------

> 由于两阶段提交和三阶段提交的种种问题，而且具体实现依赖第三方数据库，无法控制精准并发粒度，因此出现了TCC协议。

- TCC（Try-Confirm-Cancel）

TCC是Try（预留）、Confirm（确认）、Cancel（撤销）3 个操作的简称，它包含了预留、确认或撤销这2 个阶段。

Try阶段：协调者向各个执行者发送消息，检查是否能执行事务，如果可以，将相关资源冻结

Confirm阶段：如果所有执行者都可以执行，则协调者向各个执行者发送消息，执行具体的事务

Cancel阶段：如果Confirm阶段有某个执行者执行失败了，则在Cancel阶段调用对应的补偿逻辑，如果Confirm阶段执行的是扣除100元，则补偿逻辑就是将100元增加回去。

> TCC是一个补偿机制的思想，当某个操作执行失败，则在Cancel阶段执行对应的补偿操作，这是一个业务层分布式事务，通过业务代码实现。
>
> 这种方案一般使用也很少，因为事务回滚依赖于代码层面来进行回滚和补偿，会造成补偿代码巨大，而且整个代码难以维护，适合极少数金融，电商等对一致性要求非常高的场景。

---------------------------------

- 总结

二阶段提交协议，不仅仅是协议，也是一种非常经典的思想。二阶段提交在达成提交操作共识的算法中应用广泛，比如 XA 协议、TCC、Paxos、Raft等。

**Paxos、Raft 等强一致性算法，也采用了二阶段提交操作，在“提交请求阶段”，只要大多数节点确认就可以，而具有 ACID 特性的事务，则要求全部节点确认可以。**可以将具有 ACID 特性的操作，理解为最强的一致性。

### <a id="base">Base理论</a>

Base理论是ebay的架构师提出的，BASE是对CAP中的一致性和可用性权衡的结果，来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，其核心思想是**基本可用（Basically Available）和最终一致性（Eventually consistent）。**

> 除了基本可用和最终一致性之外，还提到软状态（Soft state），软状态描述的是实现服务可用性的时候系统数据的一种过渡状态，也就是说不同节点间，数据副本存在短暂的不一致。

- 基本可用

基本可用指的是分布式系统在出现不可预知的故障时，允许损失部分可用性，保证整个系统可用。

> 例如时间上的损失：延迟响应，或者功能上的损失：部分功能不可用保证核心功能可用。
>
> 例如现在最常用的流量削峰，延迟响应，体验降级，过载保护等都属于基本可用的方案。

- 最终一致性

最终一致性强调的是系统中的所有数据副本，在经过一段时间的同步后，最终能够达到一个一致性状态，不需要实时保证系统数据的一致性。

> 最终一致性方案：
>
> 读时修复：在读取数据时，检测数据的不一致，进行修复。比如 Cassandra 的 Read Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点的副本数据不一致，系统就自动修复数据。
> 写时修复：在写入数据，检测数据的不一致时，进行修复。比如 Cassandra 的 Hinted Handoff 实现。具体来说，Cassandra 集群的节点之间远程写数据的时候，如果写失败就将数据缓存下来，然后定时重传，修复数据的不一致性。
> 异步修复：这个是常用的方式，通过定时对账检测副本数据的一致性，并修复。
>
> 由于写时修复不需要做数据一致性对比，性能消耗比较低，对系统运行影响不大，因此推荐在实现最终一致性时优先实现这种方式。而读时修复和异步修复因为需要做数据的一致性对比，性能消耗比较多，在开发实际系统时，要尽量优化一致性对比的算法，降低性能消耗，避免对系统运行造成影响。
>
> 另外在实现终一致性的时候，推荐同时实现自定义写一致性级别 （All、Quorum、One、Any）， 让用户可以自主选择相应的一致性级别，比如可以通过设置一致性级别为 All，来实现强一致性。
>
> - All：强一致性，所有节点同步成功才返回客户端success
> - Quorum：写入成功的节点数达到法定人数即算成功
> - One：只要一个节点写入成功，就算成功
> - Any：没有一个节点写入成功，但请求成功保存到失败重传的缓存队列也算成功，一致性级别最低

- 总结

BASE 的核心思想是，如果不是必须的话，不推荐实现事务或强一致性，鼓励可用性和性能优先，根据业务的场景特点，来实现非常弹性的基本可用，以及实现数据的终一致性。

ACID 理论是传统数据库常用的设计理念，追求强一致性模型。BASE理论支持的是大型分布式系统，通过牺牲强一致性获得高可用性。BASE理论在很大程度上，解决了事务型系统在性能、容错、可用性等方面痛点。另外BASE 理论在 NoSQL 中应用广泛，是 NoSQL 系统设计的事实上的理论支撑。

# 二：分布式协议与算法介绍

### <a id="paxos">Paxos算法</a>

> 提到分布式算法，就不得不提 Paxos 算法，在过去几十年里，它基本上是分布式共识的代名词，因为当前最常用的一批共识算法都是基于它改进的。比如，Fast Paxos 算法、 Cheap Paxos 算法、Raft 算法、ZAB 协议等等。
>
> 兰伯特提出的 Paxos 算法包含2个部分：
>
> - 一个是 Basic Paxos 算法，描述的是多节点之间如何就某个值（提案 Value）达成共识；
> - 另一个是 Multi-Paxos 思想，描述的是执行多个Basic Paxos 实例，就一系列值达成共识。

#### Basic Paxos

> 背景：要实现一个分布式集群，这个集群是由节点 A、B、C 组成，提供只读 KV 存储服务。创建只读变量的时候，必须要对它进行赋值，这个值后续没办法修改。一个节点创建只读变量后就不能再修改它了，所以所有节点必须要先对只读变量的值达成共识，然后所有节点再一起创建这个只读变量。
> 那么，当有多个客户端（比如客户端 1、2）访问这个系统，试图创建同一个只读变量（比如X），客户端1试图创建值为3的 X，客户端2试图创建值为7的X，这样要如何达成共识，实现各节点上 X 值的一致？

Basic-Paxos算法里使用了一些独有而且比较重要的概念，提案、准备（Prepare）请求、接受（Accept）请求、角色等等。

**在Basic-Paxos算法里，角色分为三种，一个节点可以身兼多个角色：**

- 提议者（Proposer）

提议一个值，用于投票表决。在上面的示例中，客户端1和2就是提议者，但在绝大多数场景中，集群中收到客户端请求的节点才是提议者。这样做的好处是，对业务代码没有入侵性，也就是说，我们不需要在业务代码中实现算法逻辑，就可以像使用数据库一样访问后端的数据。

- 接受者（Acceptor）

对每个提议的值进行投票，并存储接受的值，比如 A、B、C 三 个节点。 一般来说，集群中的所有节点都在扮演接受者的角色，参与共识协商，并接受 和存储数据。

- 学习者（Learner）

被告知投票的结果，接受达成共识的值，存储保存，不参与投票的过程。一般来说，学习者是数据备份节点，比如“Master-Slave”模型中的 Slave，被 动地接受数据，容灾备份。

其实，这三种角色，在本质上代表的是三种功能：

- 提议者代表的是接入和协调功能，收到客户端请求后，发起二阶段提交，进行共识协商
- 接受者代表投票协商和存储数据，对提议的值进行投票，并接受达成共识的值，存储保存
- 学习者代表存储数据，不参与共识协商，只接受达成共识的值，存储备份

在 Basic Paxos 中，除了角色之外，兰伯特也使用提案代表一个提议。不过在提案中， 除了提案编号，还包含了提议值。

> 例如用[n, v]表示一个提案，其中 n 为提案编号，v 为提议值。

**在上例中，Basic-Paxos算法会如何达成共识？**

整个共识协商是分 2 个阶段进行的，假设客户端 1 的提案编号为 1，客户端 2 的提案编号为 5，并假设节点 A、B 先收到来自客户端1的准备请求，节点 C 先收到来自客户端2的准备请求。

**准备阶段：**

1. 首先客户端 1、2 作为提议者，分别向所有接受者发送包含提案编号的准备请求，在准备请求中是不需要指定提议的值的，只需要携带提案编号就可以。

2. 当节点 A、B 收到提案编号为 1 的准备请求，节点 C 收到提案编号为 5 的准备请求后，由于之前没有通过任何提案，所以节点 A、B 将返回一个 “尚无提案”的响应。也就是 说节点 A 和 B 在告诉提议者，我之前没有通过任何提案呢，并承诺以后不再响应提案编号小于等于 1 的准备请求，不会通过编号小于 1 的提案。节点 C 也是如此，它将返回一个 “尚无提案”的响应，并承诺以后不再响应提案编号小于等于 5 的准备请求，不会通过编号小于 5 的提案。
3. 当节点 A、B 收到提案编号为 5 的准备请求的时候，因为提案编号 5 大于它们之前响应的准备请求的提案编号 1，而且两个节点都没有通过任何提案，所以它将返回一个 “尚无提案”的响应，并承诺以后不再响应提案编号小于等于 5 的准备请求，不会通过编号小于 5 的提案。当节点 C 收到提案编号为 1 的准备请求的时候，由于提案编号 1 小于它之前响应的准备请求的提案编号 5，所以丢弃该准备请求，不做响应。

**接受阶段：**

1. 当客户端 1 收到大多数的接受者（节点 A、B）的准备响应后，根据响应中提案编号最大的提案的值，设置接受请求中的值。因为该值在来自节点 A、B 的准备响应中都为空（“尚无提案”），所以就把自己的提议值 3 作为提案的值，发送接受请求[1, 3]
2. 当客户端 2 收到大多数的接受者的准备响应后（节点 A、B 和节点 C），根据响应中提案编号最大的提案的值，来设置接受请求中的值。因为该值在来自节点 A、B、C 的准备响应中都为空（“尚无提案”），所以就把自己的提议值 7 作为提案的值，发送接受请求[5, 7]。
3. 当节点 A、B、C 收到接受请求[1, 3]的时候，由于提案的提案编号 1 小于三个节点承诺能通过的提案的最小提案编号 5，所以提案[1, 3]将被拒绝。
4. 当节点 A、B、C 收到接受请求[5, 7]的时候，由于提案的提案编号 5 不小于三个节点承 诺能通过的提案的最小提案编号 5，所以就通过提案[5, 7]，也就是接受了值 7，三个节点就 X 值为 7 达成了共识。

> 如果集群中有学习者，当接受者通过了一个提案时，就通知给所有的学习者。当学习者发现大多数的接受者都通过了某个提案，那么它也通过该提案，接受该提案的值。

**总结**

- Basic Paxos 是通过二阶段提交的方式来达成共识的。
- 除了共识，Basic Paxos 还实现了容错，在少于一半的节点出现故障时，集群也能工作。 它不像分布式事务算法那样，必须要所有节点都同意后才提交操作，因为“所有节点都同意”这个原则，在出现节点故障的时候会导致整个集群不可用。
- 本质上而言，提案编号的大小代表着优先级，根据提案编号的大小， 接受者保证三个承诺：如果准备请求的提案编号，小于等于接受者已经响应的准备请求的提案编号，那么接受者将承诺不响应这个准备请求；如果接受请求中的提案的提案编号，小于接受者已经响应的准备请求的提案编号，那么接受者将承诺不通过这个提案；**如果接受者之前有通过提案，那么接受者将承诺，会在准备请求的响应中， 包含已经通过的最大编号的提案信息。**

#### Multi-Paxos

> Basic Paxos 只能就单个值达成共识，一旦遇到为一系列的值实现共识的时候，就需要Multi-Paxos算法。
>
> 兰伯特提到的 Multi-Paxos 是一种思想，不是算法。而 Multi-Paxos 算法是一个统称，它是指基于 Multi-Paxos 思想，通过多个 Basic Paxos 实例实现一系列值的共识的算法（比如 Chubby 的 Multi-Paxos 实现、Raft 算法等）。 

Basic Paxos 是通过二阶段提交来达成共识的。在第一阶段，也就是准备阶段，接收到大多数准备响应的提议者，才能发起接受请求进入第二阶段（也就是接受阶段），如果我们直接通过多次执行 Basic Paxos 实例，来实现一系列值的共识，就会存在以下几个问题：

- 如果多个提议者同时提交提案，可能出现因为提案冲突，在准备阶段没有提议者接收到大多数准备响应，协商失败，需要重新协商。

>  一个 5 节点的集群，如果 3 个节点作为提议者同时提案，就可能发生因为没有提议者接收大多数响应（比如 1 个提议者接收到 1 个准备响应，另外 2 个提议者分别接收到 2 个准备响应）而准备失败，需要重新协商。

- 2 轮 RPC 通讯（准备阶段和接受阶段）往返消息多、耗性能、延迟大。

**Multi-Paxos如何解决上面的两个问题？**

- 引入领导者机制

我们可以通过引入领导者节点，也就是说，领导者节点作为唯一提议者，这样就不存在多个提议者同时提交提案的情况，也就不存在提案冲突的情况。

> 兰伯特没有说如何选举领导者，需要在实现 MultiPaxos 算法的时候自己实现。 比如在 Chubby 中，主节点（也就是领导者节点）是通过执 行 Basic Paxos 算法，进行投票选举产生的。

- 优化 Basic Paxos 执行

我们可以采用“当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段”这个优化机制，优化 Basic Paxos 执行。也就是说，领导者节点上，序列中的命令是新的，不再需要通过准备请求来发现之前被大多数节点通过的提案，领导者可以独立指定提案中的值。这时，领导者在提交命令时，可以省掉准备阶段，直接进入到接受阶段。

> 在之前的Basic Paxos中，由于客户端可以连接集群中的任意一个节点，而该节点可能之前遗漏了某些提案，导致本地数据不是最新的，因此需要一轮准备阶段，发现集群最新的值，也就是之前遗漏的提案。

和重复执行 Basic Paxos 相比，Multi-Paxos 引入领导者节点之后，因为只有领导者节点一个提议者，只有它说了算，所以就不存在提案冲突。另外，当主节点处于稳定状态时，就省掉准备阶段，直接进入接受阶段，所以在很大程度上减少了往返的消息数，提升了性能，降低了延迟。

**总结**

- Multi-Paxos只是一种思想，关于主节点选举，主节点故障重新选举，集群成员变更，读请求是否必须通过主节点等细节都没有限制，不同的算法有不同的方案。

> 限制读请求必须通过主节点，会导致集群读请求处理能力约等于单机，而且读写可能冲突，如果不限制，整个集群只保证大多数节点状态一致，不保证所有节点完全一致，因此每次读取之前都需要一个准备阶段来发现集群大多数节点对于该值的状态，从而确定值的内容。
>
> Google的Chubby限制读必须主节点。

- Multi-Paxos 引入领导者节点之后，解决了 Basic Paxos 的两个问题，但也要求所有写请求都在主节点执行，限制了集群处理写请求的并发能力，约等于单机。
- 本质上而言，“当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段”这个优化机制，是通过减少非必须的协商步骤来提升性能的。这种方法非常常用，也很有效。 比如，Google 设计的 QUIC 协议，是通过减少 TCP、TLS 的协商步骤，优化 HTTPS 性能。

### <a id="raft">Raft算法</a>

> Raft 算法属于 Multi-Paxos 算法，它是在兰伯特 Multi-Paxos 思想的基础上，做了一些简化和限制，比如增加了日志必须是连续的，只支持领导者、跟随者和候选人三种状态，在理解和算法实现上都相对容易许多。
>
> Raft 算法是现在分布式系统开发首选的共识算法。绝大多数选用 Paxos 算法的系统（比如 Cubby）都是在 Raft 算法发布前开发的，当时没得选；而全新的系统大多选择了 Raft 算法（比如 Etcd、Consul、Redis哨兵）。
>
> 从本质上说，Raft 算法是强领导者模型，通过一切以领导者为准的方式，实现一系列值的共识和各节点日志的一致。
>
> Raft算法的主要流程是在所有节点中选出主节点，由主节点来处理客户端写请求，然后通过日志复制，将数据同步给从节点，当主节点挂掉时候，重新开始选主。

#### 领导者选举

Raft 算法支持领导者（Leader）、跟随者 （Follower）和候选人（Candidate） 3 种节点身份：

- 跟随者：接收和处理来自领导者的消息，当等待领导者心跳信息超时的时候，就推荐自己当候选人。
- 候选人：候选人将向其他节点发送请求投票RPC 消息，通知其他节点来投票，如果赢得了大多数选票，就晋升当领导者。
- 领导者：一切以领导者为准，平常的主要工作内容就是 3 部分，处理写请求、管理日志复制和不断地发送心跳信息，通知其他节点“我是领导者，我还活着，你们现在不要发起新的选举，找个新领导者来替代我。”

> Raft是强领导者模型，同一时间，集群里只有一个领导者。

- 领导者选举过程

1. 初始状态下，集群中所有的节点都是跟随者的状态。
2. Raft 算法实现了随机超时时间的特性。也就是说，每个节点等待领导者节点心跳信息的超时时间间隔是随机的。
3. 当某个节点到达心跳的超时时间后，就增加自己的任期编号，并推举自己为候选人，先给自己投上一张选票，然后向其他节点发送请求投票 RPC 消息，请它们选举自己为领导者。
4. 其他节点收到请求投票的RPC消息后，检查该消息的任期信息，如果任期信息大于自己的任期并且自己在该任期内还未投票，就将自己的选票投给该节点，并更新自己的任期。
5. 如果候选人在选举超时时间内赢得了大多数的选票，那么它就会成为本届任期内新的领导者。
6. 当某个节点当选领导者后，他将周期性地发送心跳消息，通知其他服务器我是领导者，阻止跟随者发起新的选举，篡权。

- 节点间通信

在 Raft 算法中，服务器节点间的沟通联络采用的是远程过程调用（RPC），在领导者选举中，需要用到这样两类的 RPC：

1. 请求投票RPC，是由候选人在选举期间发起，通知各节点进行投票；
2. 日志复制RPC，是由领导者发起，用来复制日志和提供心跳消息。

- 任期的概念

Raft 算法中的领导者也是有任期的，每个任期由单调递增的数字标识，任期编号是随着选举的举行而变化的：

1. 跟随者在等待领导者心跳信息超时后，推举自己为候选人时，会增加自己的任期号
2. 如果一个服务器节点，发现自己的任期编号比其他节点小，那么它会更新自己的编号到较大的编号值。

Raft 算法中的任期编号的大小，会影响领导者选举和请求的处理：

1. 在 Raft 算法中约定，如果一个候选人或者领导者，发现自己的任期编号比其他节点小， 那么它会立即恢复成跟随者状态。比如分区错误恢复后，任期编号为 3 的领导者节点 B，收到来自新领导者的，包含任期编号为 4 的心跳消息，那么节点 B 将立即恢复成跟随者状态。
2. 还约定如果一个节点接收到一个包含较小的任期编号值的请求，那么它会直接拒绝这个请求。比如节点 C 的任期编号为 4，收到包含任期编号为 3 的请求投票 RPC 消息，那么它将拒绝这个消息。

> 任期的单调递增限制可以保证某些分区错误后恢复的节点发送的RPC消息不会被错误的执行。

- 选举规则

在 Raft 算法中，主要约定了以下的选举规则：

1. 领导者周期性地向所有跟随者发送心跳消息，通知大家我是领导者，阻止跟随者发起新的选举。
2. 如果在指定时间内，跟随者没有接收到来自领导者的消息，那么它就认为当前没有领导者，推举自己为候选人，发起领导者选举。
3. 在一次选举中，赢得大多数选票的候选人，将晋升为领导者。
4. 在一个任期内，领导者一直都会是领导者，直到它自身出现问题（比如宕机），或者因为网络延迟，其他节点发起一轮新的选举。
5. 在一次选举中，每一个服务器节点多会对一个任期编号投出一张选票，并且按照“先来先服务”的原则进行投票。

> 投票除了以上限制之外，当任期编号相同时，日志完整性高的跟随者拒绝将票投给日志完整性低的候选人，主要是为了尽量保证数据的完整性。

- 随机时间

在Paxos算法中，可能会出现多个候选人瓜分选票导致没有领导者产生的问题，在Multi-Paxos算法中，通过强领导者限制投票的成员数，解决这个问题，但领导者的选出仍然需要一轮Basic-Paxos算法，仍然可能出现这个问题。

Raft 算法巧妙地使用随机选举超时时间的方法，把超时时间都分散开来，在大多数情况下只有一个服务器节点先发起选举，而不是同时发起选举，这样就能减少因选票瓜分导致选举失败的情况。

> 超时超时时间包括两个地方：
>
> - 跟随者等待领导者心跳信息超时的时间间隔是随机的
> -  当没有候选人赢得过半票数，选举无效了，这时需要等待一个随机时间间隔，也就是说，等待选举超时的时间间隔是随机的

#### 日志复制

> 在 Raft 算法中，数据是以日志的形式存在的，领导者接收到来自客户端写请求后，处理写请求的过程就是一个复制和提交日志项的过程。

- 日志的概念

日志项是一种数据格式，它主要包含用户指定的数据，也就是指令，还包含一些附加信息，比如索引值、任期编号。

指令：一条由客户端请求指定的、状态机需要执行的指令。

索引值：日志项对应的整数索引值。它其实就是用来标识日志项的，是一个连续的、单调递增的整数。

任期编号：创建这条日志项的领导者的任期编号。

一届领导者任期，往往有多条日志项。而且日志项的索引值是连续的。

> 通过任期编号和日志的索引值，可以隐含的看出一个命令的发送时间。

- 日志复制

1. 首先，领导者进入第一阶段，通过日志复制RPC 消息，将日志项复制到集群其他节点上。
2. 如果领导者接收到大多数的“复制成功”响应后，它将日志项提交到它的状态机，并返回成功给客户端。如果领导者没有接收到大多数的“复制成功”响应，那么就返回错误给客户端。

> 领导者将日志项提交到它的状态机，并没有通知跟随者提交日志项，领导者不直接发送消息通知其他节点提交指定日志项。因为领导者的日志复制 RPC 消息或心跳消息，包含了当前最大的，将会被提交的日志项索引值。所以通过日志复制 RPC 消息或心跳消息，跟随者就可以知道领导者的日志提交位置信息。
>
> Raft 的日志复制其实可以理解成一个优化后的二阶段提交（将二阶段优化成了一阶段），减少了一半的往返消息，也就是降低了一半的消息延迟。

- 异常情况如何保证数据一致

> 在上面提到Raft针对两阶段提交做了优化，取消了第二步的提交动作，改由跟随者根据领导发送的RPC复制或者RPC心跳消息发现最新的提交日志，正常情况下，跟随者可以顺利提交日志，和领导者保持一致，但当出现进程崩溃，服务器宕机之类的异常情况时，领导者通过强制跟随者直接复制自己的日志项，处理不一致日志。

1. 首先，领导者通过日志复制 RPC 的一致性检查，找到跟随者节点上，与自己相同日志项的最大索引值。也就是说，这个索引值之前的日志，领导者和跟随者是一致的，之后的日志是不一致的了。
2. 然后，领导者强制跟随者更新覆盖的不一致日志项，实现日志的一致。

**具体的检查流程：**

1. 领导者向跟随者发送RPC复制日志，除了携带要复制的命令外，还携带上次日志的索引和任期编号
2. 跟随者获取领导者上一次提交日志的索引和任期编号，对比自己本地提交日志记录
   1. 如果存在，则之前所有数据一致，执行最新的命令，返回success
   2. 如果不存在，则跟随者之前丢失过数据，会返回领导者fail
3. 领导者接收跟随者响应
   1. 如果是success，则说明数据一致，RPC复制结束
   2. 如果fail，则说明跟随者之前丢失过数据，需要完成数据同步，领导者会发送上一个日志和索引和任期编号，一次一次进行对比，知道跟随者在自己本地找到日志项，返回success，领导者将缺失的日志全部发给跟随者

#### 成员变更







#### 总结

Raft 算法和Multi-Paxos不同之处，主要有2点。首先，在 Raft 中，不是所 有节点都能当选领导者，只有日志完整的节点，才能当选领导者；其次，在 Raft 中， 日志必须是连续的。

本质上，Raft 算法以领导者为中心，选举出的领导者，以“一切以我为准”的方式，达 成值的共识，和实现各节点日志的一致

### <a id="yzxhx">一致性哈希算法</a>







### <a id="gossip">Gossip协议</a>







# 三：分布式算法与协议的应用





# 四：分布式协议与算法总结

首先有拜占庭将军问题，提出分布式系统可能遇到的问题以及对应的解决方案，根据是否可能存在篡改消息，将分布式算法分为拜占庭容错算法和故障容错算法。

拜占庭容错算法主要包括：增加忠诚节点数据，可信消息签名，PBFT，PoW算法等。主要用于区块链等可能存在消息篡改的场景。

互联网常用的是故障容错算法，包括：Paxos，Raft，ZAB等。故障容错算法适合可能存在通讯故障，但不存在篡改消息的场景。

拜占庭将军问题之后的是CAP理论，该理论表明，在分布式系统中，当存在分区故障时，只能在一致性和可用性中间选择一个。

当选择CP时，可以考虑通过两阶段提交，三阶段提交，TCC等方案实现强一致性，由于CP对互联网应用太不友好，系统可用性太低，因此基于互联网长期的实践，出现了BASE理论，在一致性和可用性中间取得平衡，保证系统可用的同时达到最终一致性。

分布式常用的算法里，Basic-Paxos算法偏向于保证数据一致性，具体实现参考了两阶段提交的方式，但为了系统可用，并不保证和ACID一样，保证所有节点的数据强一致性，而是保证大多数节点一致，读取时可以通过准备阶段实现数据一致。通过牺牲部分节点数据的不一致，实现了容错的功能，集群允许少于一半的节点出现故障仍然正常工作。

Basic-Paxos算法虽然实现了数据一致性和容错性，但只是针对一个点达成共识(即客户端请求是串行化的)，想要实现并发，需要通过多个Basic-Paxos实例同时进行，但由于每个提案必须在准备阶段得到大多数节点的认可才能发起提案，多个Basic-Paxos实例同时进行可能导致提案冲突，最终没有一个提案得到大多数节点认可，除此以外，由于每次读取需要一个准备阶段来发现最新的值，因此需要两轮RPC，延迟较大。为了解决这个问题，出现了Multi-Paxos算法，Multi-Paxos提出了领导者机制，只有领导者可以发起提案，通过强领导者模型，解决了提案冲突的问题，同时优化了Basic-Paxos的流程，当领导者稳定时，不进行准备阶段，直接从领导者获取值内容。

Multi-Paxos准确地来说，应该是一种思想，因为只是提出了领导者机制，但关于主节点选举，主节点故障重新选举，集群成员变更，读请求是否必须通过主节点等细节都没有限制，因此基于Multi-Paxos思想，出现了很多实现不同，但整体思路一致的算法，如Google的Chubby和Raft等，将他们统称为Multi-Paxos算法。

Raft算法

虽然Multi-Paxos算法通过多组Basic-Paxos实例解决了并发和延迟的问题，但由于强领导者模型，整个集群的写入性能约等于单机，可以通过多组Multi-Paxos实例组成一个大的集群，客户端请求通过路由算法发到不同的Multi-Paxos实例上。由于传统的轮询，哈希等算法，在集群成员变更时，存在严重的数据迁移问题，因此出现了一致性哈希算法。







# 五：分布式事务实现方案

> 在分布式系统中，存在某些操作需要同时成功或同时失败的场景，例如转账，针对这种情况就需要分布式事务。分布式事务的实现方式很多，常见的有以下方案：
>
> 保证数据任何时刻完全一致性：2PC，TCC
>
> 保证数据最终一致性：本地消息表，可靠消息最终一致性

### <a id="2pc">2PC</a>

两阶段提交协议，将事务的划分为准备阶段和提交阶段，由一个事务管理器负责整个事务的提交和回滚。

这种分布式事务方案，比较适合单体应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，不适合高并发的场景。

这种方案可以实现数据任何时刻的完全一致性，可以通过Spring + JTA实现。

![image-20201030201950043](http://rocks526.top/lzx/image-20201030201950043.png)

### <a id="tcc">TCC</a>

TCC是一种补偿机制，核心思想是当某个事务执行失败时，对已执行的部分实现回滚和补偿操作。

这种方案在业务层面实现，对一个分布式事务里面的每个子任务都要编写对应的回滚补偿逻辑，而且业务不同，补偿逻辑也不同，对业务侵入度高，代码维护难，但可以保证数据任何时刻的完全一致性，只适合金融，电商等极少数需要数据强一致的场景。

![image-20201030202032514](http://rocks526.top/lzx/image-20201030202032514.png)

### <a id="bdxxb">本地消息表</a>

本地消息表是ebay搞出来的一种分布式事务解决方案，假设某个分布式事务依赖于A系统和B系统的操作，具体的执行流程如下：

![image-20201030202052683](http://rocks526.top/lzx/image-20201030202052683.png)

1. A系统执行事务中属于自己的部分，执行成功之后，在数据库的消息表里记录B系统要执行任务的相关消息，成功后事务完成
2. A系统通过后台的一个定时任务不断的去扫描消息表，将未执行的消息发送到MQ
3. B系统从MQ拉取消息，将要执行的任务插入自己本地的消息表中，然后执行任务，成功之后提交事务，如果失败，自己本地消息表中的数据也会回滚，如果执行成功，则通过zk通知A系统，A系统将自己本地消息表对应的记录状态改成已执行
4. 当B系统某次事务执行失败时，A系统会不断扫描本地消息表，往MQ发送消息，然后B系统不断重试，直到最终成功
5. 当B系统执行成功后，会通过zk通知A系统修改自己的本地事务表，将状态变更为已执行，之后A系统就不会再发送该条消息，分布式事务完成
6. 当B系统执行成功后，通知了A系统，但A系统还未来得及修改本地消息表，又往MQ发送一条消息，由于B系统在执行之前，会先插入本地消息表，因此只要确定一个唯一键，就可以保证幂等性。

> 这种方案是ebay对Base理论的最佳实现，是一种最终一致性思想。
>
> 系统的核心在于A系统在消息未确认之前不断发送MQ，B系统不断重复消费直到成功，MQ有可能发送重复消息，B系统需要保证幂等性，B系统成功之后通知A系统修改消息状态，可以通过zk，也可以通过调用A系统接口实现。
>
> 此方案存在的问题是严重依赖本地消息表，在高并发的场景下可能吞吐量不足。

### <a id="kkxxzzyzx">可靠消息最终一致性</a>

可靠消息最终一致性方案主要是对本地消息表方案的改良，去除了写本地消息表操作，将B系统执行失败由A系统重发消息改由MQ实现，A系统只要向MQ发送一条消息即可。这种方案需要MQ支持事务型消息，例如RocketMQ。整体流程如下：

![image-20201031105541334](http://rocks526.top/lzx/image-20201031105541334.png)

1. A系统向MQ发送pre消息，然后本地执行事务，如果执行成功，发送向MQ发送confirm消息，如果执行失败，将之前的pre消息撤回。
2. 如果A系统执行成功，但发送confirm消息失败，MQ会定时扫描所有的pre消息，回调A系统的接口，确定事务是否执行成功，A系统会再次发送一个confirm或者撤回pre消息，如此即可确保A系统完成事务的话，MQ一定会收到一条消息
3. 当A系统发送的消息confirm之后，B系统就可以消费这条消息，B系统开始执行自己本地事务，如果执行成功，向MQ提交提交这条消息，分布式事务完成
4. 如果B系统执行失败，不会提交这条消息，可以进行反复消费，反复重试直到成功。
5. 和本地消息表一样，MQ可能重复消费，B系统需要保证幂等性，之前是通过本地消息表唯一约束实现的，现在可以通过Redis或者zk实现

> 这种方式是对本地消息表的一种改良，去除了本地消息表的依赖，消息的重发依靠MQ自身实现。
>
> 在这种方案中，B系统如果一直执行失败，也可以不再执行，让B系统本地回滚，然后通过zk或其他方式通知A系统，让A系统也进行回滚，或者可以报警申请人工干预，由人工手动回滚补偿。
>
> 目前采用这种方案的比较多。

### <a id="zdnltzfa">最大努力通知方案</a>

可靠消息最终一致性方案需要依赖MQ实现事务型消息，可能有的MQ不支持，因此又引入下面这种方案，由一个单独的服务从MQ拉取消息，实现消息的重发，整体流程如下：

![image-20201031112345692](http://rocks526.top/lzx/image-20201031112345692.png)

1. 系统A执行本地事务，成功后发送消息到MQ
2. 一个专门的通知服务不断从MQ拉取消息，将消息存储到db或者内存中，然后调用B系统的接口执行任务，如果执行成功，则分布式事务完成，如果失败，则由通知服务定时重试调用B系统接口，如果一直失败，最后可以放弃事务或者人工干预

> 这个方案删除了对MQ事务型消息的依赖，由一个单独的通知服务实现消息的重试

