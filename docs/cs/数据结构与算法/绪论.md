# 一：数据结构基本概念

### 1.1 什么是数据结构

`数据`是信息的载体，是描述客观事物属性的数、字符以及所有能输入计算机中并被程序识别和处理的`符号的集合`。

`数据对象`是具有相同性质的数据元素的集合，即数据的一个子集。

`数据元素`是数据的基本单位，通常作为一个整体进行考虑和处理。

`数据项`是构成数据元素的不可分割最小单位。

`数据结构`是相互之间存在一种或多种特定关系的数据元素的集合。

数据结构三要素：

- 逻辑结构：逻辑上的结构，与现实世界映射，便于我们解决一些特定场景的问题
- 物理结构：数据在物理上的存储结构，一种逻辑结构往往有多种物理结构的实现
- 数据的运算：对集合中元素的一系列操作

### 1.2 常见的逻辑结构

数据的逻辑结构与现实世界映射，之所以抽象出逻辑结构是为了解决现实世界中的问题。常见的逻辑接口有以下几种：

- 线性结构

元素之间只有一对一的线性关系，例如现实世界中的排队场景。

> 除了线性结构外，以下几种都属于非线性结构。

- 集合

元素彼此之前没有明显的关系，只是一组同样元素的集合，例如现实世界中水果店卖的多种水果。

- 树形结构

元素之间存在一对多的关系，存在上下明显的层级结构，例如现实世界中的组织架构等。

- 图形结构

元素之间存在任意一对多、多对多关系，多个元素彼此交叉形成网状结构，例如现实世界的社交关系网。

### 1.3 常见的物理结构

数据的物理结构是其在计算机之上的实际存储方式，各种逻辑结构都有多种物理存储结构的实现，常见的有以下几种：

- 顺序存储

在内存或磁盘上开辟出连续的空间，所有元素依次排列顺序存储。

`优点是只要知道起点和元素的下标，即可快速计算出对应元素存储的位置。`

`缺点是往指定下标新增元素或删除元素时，为了保证顺序结构，后续的元素都得移动，开销很大，并且存储内存必须连续，每个元素占用大小一样，综合占用空间较大。`

- 链式存储

在内存或磁盘上链式存储，每个元素除了存储自己本身的信息之外，还存储下一个元素的地址，通过上一个元素可以找到下一个元素。

`优点是删除和新增元素时，只需要找到前一个元素，修改他们之间记录的下一个元素地址即可，并且允许内存不连续，当存在大量内存碎片时表现比顺序存储更好。`

`缺点是查询元素效率较低，只能从首节点往后遍历查找，并且由于每个元素除了存储本身的数据外，还需要存储下个元素的地址信息，对存储空间也有一定开销。`

- 索引存储

数据的存储方式可以顺序，也可以链式，也可以压缩存储，需要维护一张每个元素和地址映射的表。

`优点：当存在大量数据时，可以进行压缩存储，基于索引可以快速找到对应数据。`

`缺点：当数据存在变更时，索引也需要对应更新，且索引存储也会占用一部分空间。`

- 散列存储

元素基于某种散列函数计算得出存放的地址，散列函数保证每个元素多次计算得出的地址完全一致。

`优点：非常快速的查找性能。`

`缺点：不适合排序等，且存在散列冲突。`

### 1.4 数据的运算

数据的运算是针对集合中元素的一些操作，主要包括运算的定义和实现两部分，运算的定义是针对`逻辑结构`，运算的实现是针对`存储结构。`

# 二：算法和算法评价

### 2.1 算法的基本概念

#### 2.1.1 算法介绍

`算法`：对特定问题求解步骤的一种描述，它是指令的有限序列，其中每条指令表示一个或多个操作。

算法具备以下特点：

- 可行性
- 有穷性
- 确定性
- 有输入/输出

#### 2.1.2 算法和程序的区别

算法：算法是解决问题的一种方法或一个过程，考虑如何将输入转换成输出，一个问题可以有很多个算法。

程序：程序是某种程序设计语言对算法的具体实现。

- 算法必须是有穷的，程序可以是无穷的
- 算法必须是正确的，程序可以是错误的
- 算法可以用伪代码等描述，程序只能用代码编写并运行

### 2.2 算法的评价标准

将一个输入转换成输出，有多种算法可以实现，那么如何评价一个算法的好坏？

- 正确性：算法应该能正确求解问题
- 可读性：算法应该有良好的可读性，帮助人理解
- 健壮性：输入非法数据，算法能适应的做出反应和处理
- `效率和存储量`：效率是指算法执行时间，存储量是指算法执行过程中所需的存储空间

相比正确性、可读性、健壮性等算法必备的属性，效率和存储量更能区分一个算法的优劣。

算法的执行时间和占用的存储空间可以通过执行算法统计而得，这种方式叫做`事后统计法`，但这种方式存在很大的局限性：

- 测试结果非常依赖测试环境：Intel Core i9 处理器和 Intel Core i3 处理器的运行速度肯定不一样，甚至CPU空闲和忙碌也会导致每次运行不一致
- 测试结果受数据规模的影响很大：对同一个排序算法，待排序数据的有序度不一样，排序的执行时间就会有很大的差别，而且测试数据规模太小，测试结果可能无法真实地反应算法的性能

因此，我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。这就是时间、空间复杂度分析方法。

### 2.3 时间复杂度和空间复杂度

#### 2.3.1 O(n)表示法

时间复杂度和空间复杂度的计算方式有很多，最常用的是O(n)表示法：`T(n) = O(f(n))`。如下一段代码：

```java
 int cal(int n) {
 int sum = 0;
 int i = 1;
 for (; i <= n; ++i) {
 sum = sum + i;
 }
 return sum;
 }
```

从CPU的角度来看，这段代码的每一行都执行着类似的操作：`读数据-运算-写数据`。尽管每行代码对应的CPU执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为unit_time。在这个假设的基础之上，第 2、3行代码分别需要1个unit_time的执行时间，第4、5行都运行了n遍，所以需要2n\*unit_time的执行时间，所以这段代码总的执行时间就是(2n+2)*unit_time。可以看出来，`所有代码的执行时间T(n)与每行代码的执行次数n成正比。`

我们可以把这个规律总结成一个公式：`T(n) = O(f(n))`，这就是O(n)表示法。

这个公式中，T(n) 表示代码执行的时间；n表示数据规模的大小；f(n) 表示每行代码执行的次数总和。因为这是一个公式，所以用 f(n) 来表示。公式中的 O，表示代码的执行时间 T(n) 与 f(n) 表达式成正比。在刚才的例子中，用公式表达则为` T(n) = O(2n+2)`。

`大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势`，所以，也叫作渐进时间复杂度 （asymptotic time complexity），简称时间复杂度。

公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了。

#### 2.3.3 时间复杂度公式

前面介绍了大O时间复杂度的由来和表示方法。现在来看下，分析一段代码的时间复杂度的三个比较实用的方法：

- 只关注`循环执行次数最多`的一段代码
- `加法法则`：总复杂度等于量级最大的那段代码的复杂度
- `乘法法则`：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积

#### 2.3.4 常见时间复杂度分析

虽然代码千差万别，但是常见的复杂度量级并不多，常见的有：

<img src="http://rocks526.top/lzx/image-20210427172859860.png" alt="image-20210427172859860" style="zoom: 40%;" /><img src="http://rocks526.top/lzx/image-20210427173228884.png" alt="image-20210427173228884" style="zoom: 33%;" />

对于上面的复杂度量级，可以粗略地分为两类，`多项式量级`和`非多项式量级`。

其中，非多项式量级只有两个：O(2)和O(n!)，当数据规模n越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。

- O(1)

O(1)只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。即便执行3行，它的时间复杂度也是O(1），而不是 O(3)。总结一下，只要代码的执行时间不随n的增大而增长，这样代码的时间复杂度我们都记作O(1)。`或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。`

- O(logn)、O(nlogn)

对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。例如：

```java
i=1;
 while (i <= n) {
 i = i * 2;
 }
```

根据前面的结论，第三行代码是循环执行次数最多的。所以只要能计算出这行代码被执行了多少次，就能知道整段代码的时间复杂度。

从代码中可以看出，变量i的值从1开始取，每循环一次就乘以2。当大于n时，循环结束。变量i的取值其实是一个等比数列：

```mathematica
2^0 ==> 2^1 ==> 2^2 ····················· ==> 2^x = n
```

当i为x时，退出循环，因此求值x即可，x=log2n。所以，这段代码的时间复杂度就是O(logn)。

> 实际上，不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为O(logn)。
>
> 对数之间是可以互相转换的，log3n就等于log32*log2n，所以O(log n) = O(C * log n)，其中 C=log2是一个常量。基于我们前面的一个理论：在采用大O标记复杂度的时候，可以忽略系数，即O(Cf(n)) = O(f(n))。

理解了O(logn)，那O(nlogn) 就很容易理解了。如果一段代码的时间复杂度是O(logn)，循环执行 n 遍，时间复杂度就是O(nlogn) 了。而且，O(nlogn) 也是一种非常常见的算法时间复杂度。比如：归并排序、快速排序的时间复杂度都是O(nlogn)。

- O(m+n)、O(m*n)

O(m+n)和其他不太一样，代码的复杂度由两个数据的规模来决定。例如：

```java
int cal(int m, int n) {
 int sum_1 = 0;
 int i = 1;
 for (; i < m; ++i) {
 sum_1 = sum_1 + i;
 }
 int sum_2 = 0;
 int j = 1;
 for (; j < n; ++j) {
 sum_2 = sum_2 + j;
 }
 return sum_1 + sum_2;
}
```

从代码中可以看出，m和n是表示两个数据规模。我们无法事先评估m和n谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是O(m+n)。

> 针对这种情况，原来的加法法则就不正确了，需要将加法规则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则继续有效：T1(m)*T2(n) = O(f(m) * f(n))。

#### 2.3.5 空间复杂度

时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，`空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。`

我们常见的空间复杂度就是 O(1)、O(n)、O(n )，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。

#### 2.3.6 最好、最坏、平均、均摊复杂度

- 最好、最坏时间复杂度

```java
    // n 表示数组 array 的长度
    int find(int[] array, int n, int x) {
        int i = 0;
        int pos = -1;
        for (; i < n; ++i) {
            if (array[i] == x) {
                pos = i;
                break;
            }
        }
        return pos;
    }
```

这段代码要实现的功能是，在一个无序的数组（array）中，查找变量 x 出现的位置。如果没有找到，就返回 -1。

在这块代码中，要查找的变量 x 可能出现在数组的任意位置。如果数组中第一个元素正好是要查找的变量 x，那就不需要继续遍历剩下的 n-1 个数据了，那时间复杂度就是 O(1)。但如果数组中不存在变量 x，那我们就需要把整个数组都遍历一遍，时间复杂度就成了 O(n)。所以，不同的情况下，这段代码的时间复杂度是不一样的。

为了表示代码在不同情况下的不同时间复杂度，我们需要引入两个概念：`最好情况时间复杂度、最坏情况时间复杂度`

`最好情况时间复杂度：在最理想的情况下，执行这段代码的时间复杂度，上例为O(1)。`

`最坏情况时间复杂度：在最糟糕的情况下，执行这段代码的时间复杂度，上例为O(n)。`

- 平均时间复杂度

我们都知道，最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，我们需要引入另一个概念：`平均情况时间复杂度，后面我简称为平均时间复杂度。`下面以刚才的代码为例分析平均时间复杂度：

要查找的变量 x 在数组中的位置，有 n+1 种情况：在数组的 0～n-1 位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即：

```mathematica
(1+2+3+······+n+n)/(n+1) = n(n+3)/2(n+1)
```

时间复杂度的大 O 标记法中，可以省略掉系数、低阶、常量，所以把这个公式简化之后，得到的平均时间复杂度就是 O(n)。

这个结论虽然是正确的，但是计算过程稍微有点儿问题：`刚说的这 n+1 种情况，出现的概率并不是一样的。`

要查找的变量 x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便理解，假设在数组中与不在数组中的概率都为 1/2。另外，要查找的数据出现在 0～n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据概率乘法法则，要查找的数据出现在 0～n-1 中任意位置的概率就是 1/(2n)。最终计算公式为：

```mathematica
1x(1/2n)+2x(1/2n)+3x(1/2n)+·······+nx(1/2x)+nx(1/2) = (3n+1)/4
```

`这个值就是概率论中的加权平均值，也叫作期望值，所以平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度。`

引入概率之后，前面那段代码的加权平均值为 (3n+1)/4。用大 O 表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是O(n)。

- 均摊时间复杂度

大部分情况下，并不需要区分最好、最坏、平均三种复杂度。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限。

```java
// array 表示一个长度为 n 的数组
 // 代码中的 array.length 就等于 n
 int[] array = new int[n];
 int count = 0;
 void insert(int val) {
 if (count == array.length) {
 int sum = 0;
 for (int i = 0; i < array.length; ++i) {
 sum = sum + array[i];
 }
 array[0] = sum;
 count = 1;
 }
 array[count] = val;
 ++count;
 }
```

这段代码实现了一个往数组中插入数据的功能。当数组满了之后，遍历数组求和，并清空数组，将求和之后的 sum 值放到数组的第一个位置，然后再将新的数据插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。

最理想的情况下，数组中有空闲空间，我们只需要将数据插入到数组下标为 count 的位置就可以了，`所以最好情况时间复杂度为 O(1)`。最坏的情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，`所以最坏情况时间复杂度为 O(n)。平均时间复杂度根据前面的公式计算，最后结果为O(1)。`

```mathematica
1x(1/(n+1)) + 1x(1/(n+1)) + ·················· + 1x(1/(n+1)) + nx(1/(n+1)) = O(1)
```

这个例子中，其实平均时间复杂度分析不用这么复杂，对比前面的代码，前面的查找的时间复杂度在极端情况下才为O(1)，而插入这个函数的时间复杂度在极端情况下才为O(n)，大部分为O(1)。而且O(1) 时间复杂度的插入和 O(n) 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟着 n-1 个 O(1) 的插入操作，循环往复。因此，`我们将O(n)时间复杂度的那次操作均摊到其他n-1次的O(1)复杂度的操作上，最终复杂度就是O(1)。`

`针对这种特殊的场景，我们引入了一种更加简单的分析方法：摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字，叫均摊时间复杂度。`

# 三：学习路线

![数据结构与算法 (1)](http://rocks526.top/lzx/数据结构与算法 (1).png)

数据结构和算法的内容很多，常用的有以下20个：

- 数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie、树。
- 算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。