- [分布式理论](#ll)
  - [拜占庭将军问题](#bzt)
  - [CAP理论](#cap)
  - [ACID理论](#acid)
  - [BASE理论](#base)
- [分布式常用算法](#sf)
  - [Paxos算法](#paxos)
  - [Raft算法](#raft)
  - [一致性哈希算法](#yzxhx)
  - [Gossip协议](#gossip)
  - [QuorumNWR算法](#quorumNWR)
  - [PBFT算法](#pbft)
  - [PoW算法](#pow)
  - [ZAB协议](#zab)
- [分布式算法实战](#sz)
- [分布式事务](#sw)
  - [2PC](#2pc)
  - [TCC](#tcc)
  - [本地消息表](#bdxxb)
  - [可靠消息最终一致性](#kkxxzzyzx)
  - [最大努力通知方案](#zdnltzfa)

-------------------------

# <a id="ll">分布式理论</a>

### <a id="bzt">拜占庭将军问题</a>

> 拜占庭将军问题是借一个故事展示分布式共识的问题，同时讨论了分布式达成共识的解决方法。

- 故事背景

在很久很久以前，拜占庭是东罗马帝国的首都。那个时候罗马帝国国土辽阔，为了防御目的，因此每个军队都分隔很远，将军与将军之间只能靠信使传递消息。

在打仗的时候，拜占庭军队内所有将军必需达成**一致的共识**，才能更好地赢得胜利。但是，在军队内有可能存有叛徒，扰乱将军们的决定，并且信使传递过程中也可能被拦截甚至被间谍替换。

这时候，在已知有成员不可靠的情况下，其余忠诚的将军需要在不受叛徒或间谍的影响下达成一致的协议。

莱斯利·兰伯特（ Leslie Lamport ）通过这个比喻，表达了计算机网络中所存在的一致性问题。这个问题被称为**拜占庭将军问题**。

- 两忠一叛难题

三位将军商量最终作战方案，其中两位忠诚，一位是叛徒。

- 解决方案

1. **口信消息型拜占庭将军问题之解**

首先所有将军都设置没有接到命令时的默认命令，然后随机一个将军向所有将军发送要执行的命令，之后其他所有将军将自己收到的命令发送给别的将军，最终所有将军执行收到的数量多的命令。

> 达成一致的前提条件：如果叛徒的数量是M，将军人数不能少于3M+1，并且需要M轮协商。
>
> 通过增加忠诚将军的数量实现最终一致性。

2. **签名消息型拜占庭问题之解**

还可以通过签名的方式，在不增加将军人数的情况下，解决二忠一叛的难题。签名需要具备以下特点：

忠诚将军的签名无法伪造，而且对他签名消息的内容进行任何更改都会被发现；
任何人都能验证将军签名的真伪。

当忠诚将军发现签名被伪造时，即知道该消息是叛徒发送，则执行另外忠诚将军发送的命令。

- 总结

> 将军 ==>  计算机节点
>
> 忠诚将军  ==>  运行正常的计算机节点
>
> 叛变将军  ==>  出现故障并且会发送误导信息的节点
>
> 信使被杀  ==>   通信故障，消息丢失
>
> 信使被间谍替换  ==>  通信被中间人攻击，信息被伪造

拜占庭将军问题描述的是困难的，也是复杂的一种分布式故障场 景，除了存在故障行为，还存在恶意行为的一个场景。

在存在恶意节点行为的场景中（比如在数字货币的区块链技术中），必须使用拜占庭容错算法（Byzantine Fault Tolerance，BFT）。除了上面提到两种算法，常用的拜占庭容错算法还有：**PBFT算法，PoW算法**。

而在计算机分布式系统中，常用的是非拜占庭容错算法，即故障容错算法（Crash Fault Tolerance，CFT）。CFT 解决的是分布式的系统中存在故障，但不存在恶意节点的场景下的共识问题。 也就是说，这个场景可能会丢失消息，或者有消息重复，但不存在错误消 息，或者伪造消息的情况。常见的算法有**Paxos算法、Raft算法、ZAB协议。**

### <a id="cap">CAP理论</a>

CAP理论是对分布式系统的特性做出的抽象，总结出一致性、可用性和分区容错性三个特性。**而在分布式系统中，这三个特性不可兼得，只能选择其中两个。**

- 一致性（Consistency）

一致性说的是客户端的每次读操作，不管访问哪个节点，要么读到的都是同一份最新的数据，要么读取失败。

> 一致性强调的是各节点的数据一致性，而不是数据完整性。

- 可用性（Availability）

可用性说的是任何来自客户端的请求，不管访问哪个节点，都能得到响应数据，但不保证是同一份最新数据。

也可以把可用性看作是分布式系统对访问本系统的客户端的另外一种承诺：我尽力给你返回数据，不会不响应你，但是我不保证每个节点给你的数据都是最新的。 

这个指标强调的是服务可用，但不保证数据的一致。

- 分区容错性（Partition Tolerance）

分区容错性说的是当节点间出现任意数量的消息丢失或高延迟的时候，系统仍然可以继续提供服务。

也就是说，分布式系统在告诉访问本系统的客户端：不管我的内部出现什么样的数据同步问题，我会一直运行，提供服务。

这个指标，强调的是集群对分区故障的容错能力。

> 因为分布式系统与单机系统不同，它涉及到多节点间的通讯和交互，节点间的分区故障是必然发生的，因此分区容错性必须满足，一般在可用性和数据一致性之间做出选择。

CP：当选择了一致性（C）的时候，如果因为消息丢失、延迟过高发生了网络分区，部分节点无法保证特定信息是最新的，那么这个时候，当集群节点接收到来自客户端的写请求时，因为无法保证所有节点都是最新信息，所以系统将返回写失败错误，也就是说集群拒绝新数据写入。
AP：当选择了可用性（A）的时候，系统将始终处理客户端的查询，返回特定信息，如果发生 了网络分区，一些节点将无法返回最新的特定信息，它们将返回自己当前的相对新的信息。

> 并非在任何情况下，分布式系统都只能在AC中选择一个，当系统正常运行时(没有产生网络分区时)，P是不需要被满足的，此时是可以同时保证AC的。只有当系统网络出现分区，必须满足P时，AC只能选择一个。
>

- 总结

CA 模型：在分布式系统中不存在。因为舍弃 P，意味着舍弃分布式系统，就比如单机版关系型数据库MySQL，如果MySQL要考虑主备或集群部署时，它必须考虑P。

CP 模型：采用CP模型的分布式系统，一旦因为消息丢失、延迟过高发生了网络分区， 就影响用户的体验和业务的可用性。因为为了防止数据不一致，集群将拒绝新数据的写入，典型的应用是ZooKeeper，MongoDB，Etcd 和 HBase。

AP 模型：采用 AP 模型的分布式系统，实现了服务的高可用。用户访问系统的时候，都能得到响应数据，不会出现响应错误，但当出现分区故障时，相同的读操作，访问不同的节点，得到响应数据可能不一样。典型应用就比如Cassandra 和DynamoDB。

### <a id="acid">ACID理论</a>

事务是指访问并可能更新数据库中各种数据项的一个程序执行单元(unit)，通过事务，可以保证数据库的数据总是处于一个一致性的状态下。事务具备ACID特性：

- 原子性（atomicity)

一个事务要么全部提交成功，要么全部失败回滚，不能只执行其中的一部分操作

- 一致性（consistency)

事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行之前和执行之后，数据库都必须处于一致性状态。

> 如果数据库系统在运行过程中发生故障，有些事务尚未完成就被迫中断，这些未完成的事务对数据库所作的修改有一部分已写入物理数据库，这是数据库就处于一种不正确的状态，也就是不一致的状态。
>
> 此时事务会通过回滚让数据库重新回到一致性状态。

- 隔离性（isolation）

事务的隔离性是指在并发环境中，并发的事务是相互隔离的，一个事务的执行不能被其他事务干扰。

不同的事务并发操作相同的数据时，每个事务都有各自完成的数据空间，即一个事务内部的操作及使用的数据对其他并发事务是隔离的，并发执行的各个事务之间不能相互干扰。

> 在标准SQL规范中，定义了4个事务隔离级别，不同的隔离级别对事务的处理不同，分别是：
>
> - 读未提交（Read Uncommited）：当多个事务并发时，某个事务可以读取其他事务修改后的数据，即便该事务还没有提交
> - 已提交读（Read Commited）：当多个事务并发时，某个事务只能读取其他事务提交后的数据
> - 可重复读（Repeatable Read)：保证在事务处理过程中，多次读取同一个数据时，其值都和事务开始时刻是一致的(解决某个事务执行过程中，别的事务提交了修改数据，导致该事务两次读取不一致)
> - 串行化：最严格的事务隔离级别，它要求所有事务被串行执行，即事务只能一个接一个的进行处理，不能并发执行。
>
> 不同数据库对RR级别实现不一致，导致有的可以解决幻读，有的不能解决幻读。

- 持久化（durability）

一旦事务提交，那么它对数据库中的对应数据的状态的变更就会永久保存到数据库中。--即使发生系统崩溃或机器宕机等故障，只要数据库能够重新启动，那么一定能够将其恢复到事务成功结束的状态。

> 可以认为，实现了事务，就可以保证数据库安全，即数据一致性。在单机可以通过加锁，时间序列，MVCC等机制实现事务，在分布式环境，想要实现事务需要通过分布式事务协议，如二阶段提交协议，三阶段提交协议和TCC协议。

-------------------------

- 二阶段提交协议

分布式事务的过程是将事务发送到多个节点，多个节点执行属于自己的一部分，如果有任意一个节点执行失败，则整个事务得回滚，涉及多个节点，而且需要一个角色收集所有节点的处理情况，实现繁琐。

因此出现了二阶段提交协议，二阶段协议提出了协调者的概念，由协调者负责收集各个节点的处理情况，向客户端返回响应。

除此以外，在正式执行事务之前，还加入一个准备阶段，将事务发送给各个节点，各节点判断是否可执行，如果可执行，则锁定资源并返回协调者yes，协调者收到所有yes再让各节点提交事务，如果有一个节点不是yes，则通知各节点不执行事务，释放锁定的资源。

> 二阶段提交协议整体上只是一种思想，核心在于两点：
>
> - 通过引入协调者，实现各节点处理状态的收集
> - 在正式执行之前加入一个准备阶段，统计事务能否进行执行
>
> 二阶段提交协议如果要在工程上实现，还有很多细节需要补充，如协调者决定提交事务后，发送给各节点提交事物的消息丢失，或者有节点宕机如何处理。
>
> 在二阶段提交协议中，只是规定各节点只要返回协调者yes，即便之后消息丢失或者宕机也要能提交事务，具体如何实现，有不同的方式。
>
> 参考：https://blog.csdn.net/lengxiao1993/article/details/88290514

二阶段提交协议最早是用来实现数据库的分布式事务的，不过现在最常用的协议是XA协议。这个协议是 X/Open国际联盟基于二阶段提交协议提出的，也叫作X/Open Distributed Transaction Processing（DTP）模型，比如MySQL就是通过MySQL XA实现了分布式事务。

无论是二阶段提交协议还是XA协议，都存在一些问题：

1. 协调者存在单点故障问题
2. 在准备阶段需要预留资源，将事务涉及的相关资源锁定，当协调者挂掉，一直不发送commit时，执行者将资源一直锁定
3. 事务状态丢失的问题，即便协调者做双机热备，挂掉自动选举出新的协调者，当协调者和某个执行者同时挂掉的时候，新的协调者无法得知挂掉的执行者是佛偶执行了commit
4. 脑裂问题：当由于网络原因，部分执行者没有收到commit消息，而其他执行者收到了commit消息，就会导致数据不一致

--------------------------------

- 三阶段提交协议

为了缓解和优化两阶段提交协议存在的问题，出现了三阶段提交协议。将整个分布式事务分为canCommit，preCommit，doCommit三个阶段进行，同时引入超时机制和默认操作机制。

canCommit阶段：检查各个执行者状态，是否可以执行事务

preCommit阶段：预留资源，将事务涉及的资源进行锁定，类似于两阶段提交的准备阶段

doCommit阶段：通知各个执行者执行事务

> 为了解决单点问题，引入超时机制和默认操作，当协调者挂掉时，执行者长期等不到commit执行，会自行执行默认的commit，同时解决资源长期占用问题。
>
> 引入canCommit阶段的目的在于，只有canCommit全部返回yes，才能发送preCommit指令，当协调者超时后，大家都默认commit，在两阶段提交协议里，只有一个准备阶段，执行者无法得知其他执行者是否可执行。
>
> 三阶段提交同样存在脑裂问题，当协调者在doCommit阶段，不发送commit指令，而是发送回滚指令，如果某个执行者出现脑裂，会无法收到消息，执行commit导致数据不一致
>
> 无论是两阶段提交还是三阶段提交，一般都适合在单机操作多个数据库的场景，而在现在的微服务架构下，基本是每个服务对应自己的库，操作别的库需要调用别的服务接口，一般不存在这种场景，因此使用很少，基本是在数据库层面实现的。

-------------------------------

> 由于两阶段提交和三阶段提交的种种问题，而且具体实现依赖第三方数据库，无法控制精准并发粒度，因此出现了TCC协议。

- TCC（Try-Confirm-Cancel）

TCC是Try（预留）、Confirm（确认）、Cancel（撤销）3 个操作的简称，它包含了预留、确认或撤销这2 个阶段。

Try阶段：协调者向各个执行者发送消息，检查是否能执行事务，如果可以，将相关资源冻结

Confirm阶段：如果所有执行者都可以执行，则协调者向各个执行者发送消息，执行具体的事务

Cancel阶段：如果Confirm阶段有某个执行者执行失败了，则在Cancel阶段调用对应的补偿逻辑，如果Confirm阶段执行的是扣除100元，则补偿逻辑就是将100元增加回去。

> TCC是一个补偿机制的思想，当某个操作执行失败，则在Cancel阶段执行对应的补偿操作，这是一个业务层分布式事务，通过业务代码实现。
>
> 这种方案一般使用也很少，因为事务回滚依赖于代码层面来进行回滚和补偿，会造成补偿代码巨大，而且整个代码难以维护，适合极少数金融，电商等对一致性要求非常高的场景。

---------------------------------

- 总结

二阶段提交协议，不仅仅是协议，也是一种非常经典的思想。二阶段提交在达成提交操作共识的算法中应用广泛，比如 XA 协议、TCC、Paxos、Raft等。

**Paxos、Raft 等强一致性算法，也采用了二阶段提交操作，在“提交请求阶段”，只要大多数节点确认就可以，而具有 ACID 特性的事务，则要求全部节点确认可以。**可以将具有 ACID 特性的操作，理解为最强的一致性。

### <a id="base">Base理论</a>

Base理论是ebay的架构师提出的，BASE是对CAP中的一致性和可用性权衡的结果，来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，其核心思想是**基本可用（Basically Available）和最终一致性（Eventually consistent）。**

> 除了基本可用和最终一致性之外，还提到软状态（Soft state），软状态描述的是实现服务可用性的时候系统数据的一种过渡状态，也就是说不同节点间，数据副本存在短暂的不一致。

- 基本可用

基本可用指的是分布式系统在出现不可预知的故障时，允许损失部分可用性，保证整个系统可用。

> 例如时间上的损失：延迟响应，或者功能上的损失：部分功能不可用保证核心功能可用。
>
> 例如现在最常用的流量削峰，延迟响应，体验降级，过载保护等都属于基本可用的方案。

- 最终一致性

最终一致性强调的是系统中的所有数据副本，在经过一段时间的同步后，最终能够达到一个一致性状态，不需要实时保证系统数据的一致性。

> 最终一致性方案：
>
> 读时修复：在读取数据时，检测数据的不一致，进行修复。比如 Cassandra 的 Read Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点的副本数据不一致，系统就自动修复数据。
> 写时修复：在写入数据，检测数据的不一致时，进行修复。比如 Cassandra 的 Hinted Handoff 实现。具体来说，Cassandra 集群的节点之间远程写数据的时候，如果写失败就将数据缓存下来，然后定时重传，修复数据的不一致性。
> 异步修复：这个是常用的方式，通过定时对账检测副本数据的一致性，并修复。
>
> 由于写时修复不需要做数据一致性对比，性能消耗比较低，对系统运行影响不大，因此推荐在实现最终一致性时优先实现这种方式。而读时修复和异步修复因为需要做数据的一致性对比，性能消耗比较多，在开发实际系统时，要尽量优化一致性对比的算法，降低性能消耗，避免对系统运行造成影响。
>
> 另外在实现终一致性的时候，推荐同时实现自定义写一致性级别 （All、Quorum、One、Any）， 让用户可以自主选择相应的一致性级别，比如可以通过设置一致性级别为 All，来实现强一致性。
>
> - All：强一致性，所有节点同步成功才返回客户端success
> - Quorum：写入成功的节点数达到法定人数即算成功
> - One：只要一个节点写入成功，就算成功
> - Any：没有一个节点写入成功，但请求成功保存到失败重传的缓存队列也算成功，一致性级别最低

- 总结

BASE 的核心思想是，如果不是必须的话，不推荐实现事务或强一致性，鼓励可用性和性能优先，根据业务的场景特点，来实现非常弹性的基本可用，以及实现数据的终一致性。

ACID 理论是传统数据库常用的设计理念，追求强一致性模型。BASE理论支持的是大型分布式系统，通过牺牲强一致性获得高可用性。BASE理论在很大程度上，解决了事务型系统在性能、容错、可用性等方面痛点。另外BASE 理论在 NoSQL 中应用广泛，是 NoSQL 系统设计的事实上的理论支撑。

# <a id="sf">分布式常用算法</a>

### <a id="paxos">Paxos算法</a>

> 提到分布式算法，就不得不提 Paxos 算法，在过去几十年里，它基本上是分布式共识的代名词，因为当前最常用的一批共识算法都是基于它改进的。比如，Fast Paxos 算法、 Cheap Paxos 算法、Raft 算法、ZAB 协议等等。
>
> 兰伯特提出的 Paxos 算法包含2个部分：
>
> - 一个是 Basic Paxos 算法，描述的是多节点之间如何就某个值（提案 Value）达成共识；
> - 另一个是 Multi-Paxos 思想，描述的是执行多个Basic Paxos 实例，就一系列值达成共识。

#### Basic Paxos

> 背景：要实现一个分布式集群，这个集群是由节点 A、B、C 组成，提供只读 KV 存储服务。创建只读变量的时候，必须要对它进行赋值，这个值后续没办法修改。一个节点创建只读变量后就不能再修改它了，所以所有节点必须要先对只读变量的值达成共识，然后所有节点再一起创建这个只读变量。
> 那么，当有多个客户端（比如客户端 1、2）访问这个系统，试图创建同一个只读变量（比如X），客户端1试图创建值为3的 X，客户端2试图创建值为7的X，这样要如何达成共识，实现各节点上 X 值的一致？

Basic-Paxos算法里使用了一些独有而且比较重要的概念，提案、准备（Prepare）请求、接受（Accept）请求、角色等等。

**在Basic-Paxos算法里，角色分为三种，一个节点可以身兼多个角色：**

- 提议者（Proposer）

提议一个值，用于投票表决。在上面的示例中，客户端1和2就是提议者，但在绝大多数场景中，集群中收到客户端请求的节点才是提议者。这样做的好处是，对业务代码没有入侵性，也就是说，我们不需要在业务代码中实现算法逻辑，就可以像使用数据库一样访问后端的数据。

- 接受者（Acceptor）

对每个提议的值进行投票，并存储接受的值，比如 A、B、C 三 个节点。 一般来说，集群中的所有节点都在扮演接受者的角色，参与共识协商，并接受 和存储数据。

- 学习者（Learner）

被告知投票的结果，接受达成共识的值，存储保存，不参与投票的过程。一般来说，学习者是数据备份节点，比如“Master-Slave”模型中的 Slave，被 动地接受数据，容灾备份。

其实，这三种角色，在本质上代表的是三种功能：

- 提议者代表的是接入和协调功能，收到客户端请求后，发起二阶段提交，进行共识协商
- 接受者代表投票协商和存储数据，对提议的值进行投票，并接受达成共识的值，存储保存
- 学习者代表存储数据，不参与共识协商，只接受达成共识的值，存储备份

在 Basic Paxos 中，除了角色之外，兰伯特也使用提案代表一个提议。不过在提案中， 除了提案编号，还包含了提议值。

> 例如用[n, v]表示一个提案，其中 n 为提案编号，v 为提议值。

**在上例中，Basic-Paxos算法会如何达成共识？**

整个共识协商是分 2 个阶段进行的，假设客户端 1 的提案编号为 1，客户端 2 的提案编号为 5，并假设节点 A、B 先收到来自客户端1的准备请求，节点 C 先收到来自客户端2的准备请求。

**准备阶段：**

1. 首先客户端 1、2 作为提议者，分别向所有接受者发送包含提案编号的准备请求，在准备请求中是不需要指定提议的值的，只需要携带提案编号就可以。

2. 当节点 A、B 收到提案编号为 1 的准备请求，节点 C 收到提案编号为 5 的准备请求后，由于之前没有通过任何提案，所以节点 A、B 将返回一个 “尚无提案”的响应。也就是 说节点 A 和 B 在告诉提议者，我之前没有通过任何提案呢，并承诺以后不再响应提案编号小于等于 1 的准备请求，不会通过编号小于 1 的提案。节点 C 也是如此，它将返回一个 “尚无提案”的响应，并承诺以后不再响应提案编号小于等于 5 的准备请求，不会通过编号小于 5 的提案。
3. 当节点 A、B 收到提案编号为 5 的准备请求的时候，因为提案编号 5 大于它们之前响应的准备请求的提案编号 1，而且两个节点都没有通过任何提案，所以它将返回一个 “尚无提案”的响应，并承诺以后不再响应提案编号小于等于 5 的准备请求，不会通过编号小于 5 的提案。当节点 C 收到提案编号为 1 的准备请求的时候，由于提案编号 1 小于它之前响应的准备请求的提案编号 5，所以丢弃该准备请求，不做响应。

**接受阶段：**

1. 当客户端 1 收到大多数的接受者（节点 A、B）的准备响应后，根据响应中提案编号最大的提案的值，设置接受请求中的值。因为该值在来自节点 A、B 的准备响应中都为空（“尚无提案”），所以就把自己的提议值 3 作为提案的值，发送接受请求[1, 3]
2. 当客户端 2 收到大多数的接受者的准备响应后（节点 A、B 和节点 C），根据响应中提案编号最大的提案的值，来设置接受请求中的值。因为该值在来自节点 A、B、C 的准备响应中都为空（“尚无提案”），所以就把自己的提议值 7 作为提案的值，发送接受请求[5, 7]。
3. 当节点 A、B、C 收到接受请求[1, 3]的时候，由于提案的提案编号 1 小于三个节点承诺能通过的提案的最小提案编号 5，所以提案[1, 3]将被拒绝。
4. 当节点 A、B、C 收到接受请求[5, 7]的时候，由于提案的提案编号 5 不小于三个节点承 诺能通过的提案的最小提案编号 5，所以就通过提案[5, 7]，也就是接受了值 7，三个节点就 X 值为 7 达成了共识。

> 如果集群中有学习者，当接受者通过了一个提案时，就通知给所有的学习者。当学习者发现大多数的接受者都通过了某个提案，那么它也通过该提案，接受该提案的值。

**总结**

- Basic Paxos 是通过二阶段提交的方式来达成共识的。
- 除了共识，Basic Paxos 还实现了容错，在少于一半的节点出现故障时，集群也能工作。 它不像分布式事务算法那样，必须要所有节点都同意后才提交操作，因为“所有节点都同意”这个原则，在出现节点故障的时候会导致整个集群不可用。
- 本质上而言，提案编号的大小代表着优先级，根据提案编号的大小， 接受者保证三个承诺：如果准备请求的提案编号，小于等于接受者已经响应的准备请求的提案编号，那么接受者将承诺不响应这个准备请求；如果接受请求中的提案的提案编号，小于接受者已经响应的准备请求的提案编号，那么接受者将承诺不通过这个提案；**如果接受者之前有通过提案，那么接受者将承诺，会在准备请求的响应中， 包含已经通过的最大编号的提案信息。**

#### Multi-Paxos

> Basic Paxos 只能就单个值达成共识，一旦遇到为一系列的值实现共识的时候，就需要Multi-Paxos算法。
>
> 兰伯特提到的 Multi-Paxos 是一种思想，不是算法。而 Multi-Paxos 算法是一个统称，它是指基于 Multi-Paxos 思想，通过多个 Basic Paxos 实例实现一系列值的共识的算法（比如 Chubby 的 Multi-Paxos 实现、Raft 算法等）。 

Basic Paxos 是通过二阶段提交来达成共识的。在第一阶段，也就是准备阶段，接收到大多数准备响应的提议者，才能发起接受请求进入第二阶段（也就是接受阶段），如果我们直接通过多次执行 Basic Paxos 实例，来实现一系列值的共识，就会存在以下几个问题：

- 如果多个提议者同时提交提案，可能出现因为提案冲突，在准备阶段没有提议者接收到大多数准备响应，协商失败，需要重新协商。

>  一个 5 节点的集群，如果 3 个节点作为提议者同时提案，就可能发生因为没有提议者接收大多数响应（比如 1 个提议者接收到 1 个准备响应，另外 2 个提议者分别接收到 2 个准备响应）而准备失败，需要重新协商。

- 2 轮 RPC 通讯（准备阶段和接受阶段）往返消息多、耗性能、延迟大。

**Multi-Paxos如何解决上面的两个问题？**

- 引入领导者机制

我们可以通过引入领导者节点，也就是说，领导者节点作为唯一提议者，这样就不存在多个 提议者同时提交提案的情况，也就不存在提案冲突的情况。

> 兰伯特没有说如何选举领导者，需要在实现 MultiPaxos 算法的时候自己实现。 比如在 Chubby 中，主节点（也就是领导者节点）是通过执 行 Basic Paxos 算法，进行投票选举产生的。

- 优化 Basic Paxos 执行

我们可以采用“当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段”这个优化机制，优化 Basic Paxos 执行。也就是说，领导者节点上，序列中的命令是新的，不再需要通过准备请求来发现之前被大多数节点通过的提案，领导者可以独立指定提案中的值。这时，领导者在提交命令时，可以省掉准备阶段，直接进入到接受阶段。

> 在之前的Basic Paxos中，由于客户端可以连接集群中的任意一个节点，而该节点可能之前遗漏了某些提案，导致本地数据不是最新的，因此需要一轮准备阶段，发现集群最新的值，也就是之前遗漏的提案。

和重复执行 Basic Paxos 相比，Multi-Paxos 引入领导者节点之后，因为只有领导者节点一个提议者，只有它说了算，所以就不存在提案冲突。另外，当主节点处于稳定状态时，就省掉准备阶段，直接进入接受阶段，所以在很大程度上减少了往返的消息数，提升了性能，降低了延迟。

**总结**

- Multi-Paxos只是一种思想，关于主节点选举，主节点故障重新选举，集群成员变更，读请求是否必须通过主节点等细节都没有限制，不同的算法有不同的方案。

> 限制读请求必须通过主节点，会导致集群读请求处理能力约等于单机，而且读写可能冲突，如果不限制，整个集群只保证大多数节点状态一致，不保证所有节点完全一致，可能读取到脏数据。
>
> Google的Chubby限制读必须主节点。

- Multi-Paxos 引入领导者节点之后，解决了 Basic Paxos 的两个问题，但也要求所有写请求都在主节点执行，限制了集群处理写请求的并发能力，约等于单机。
- 本质上而言，“当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段”这个优化机制，是通过减少非必须的协商步骤来提升性能的。这种方法非常常用，也很有效。 比如，Google 设计的 QUIC 协议，是通过减少 TCP、TLS 的协商步骤，优化 HTTPS 性能。

### <a id="raft">Raft算法</a>









### <a id="yzxhx">一致性哈希算法</a>







### <a id="gossip">Gossip协议</a>







# <a id="sz">分布式算法实战</a>





# <a id="sw">分布式事务</a>

> 在分布式系统中，存在某些操作需要同时成功或同时失败的场景，例如转账，针对这种情况就需要分布式事务。分布式事务的实现方式很多，常见的有以下方案：
>
> 保证数据任何时刻完全一致性：2PC，TCC
>
> 保证数据最终一致性：本地消息表，可靠消息最终一致性

### <a id="2pc">2PC</a>

两阶段提交协议，将事务的划分为准备阶段和提交阶段，由一个事务管理器负责整个事务的提交和回滚。

这种分布式事务方案，比较适合单体应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，不适合高并发的场景。

这种方案可以实现数据任何时刻的完全一致性，可以通过Spring + JTA实现。

![image-20201030201950043](http://rocks526.top/lzx/image-20201030201950043.png)

### <a id="tcc">TCC</a>

TCC是一种补偿机制，核心思想是当某个事务执行失败时，对已执行的部分实现回滚和补偿操作。

这种方案在业务层面实现，对一个分布式事务里面的每个子任务都要编写对应的回滚补偿逻辑，而且业务不同，补偿逻辑也不同，对业务侵入度高，代码维护难，但可以保证数据任何时刻的完全一致性，只适合金融，电商等极少数需要数据强一致的场景。

![image-20201030202032514](http://rocks526.top/lzx/image-20201030202032514.png)

### <a id="bdxxb">本地消息表</a>

本地消息表是ebay搞出来的一种分布式事务解决方案，假设某个分布式事务依赖于A系统和B系统的操作，具体的执行流程如下：

![image-20201030202052683](http://rocks526.top/lzx/image-20201030202052683.png)

1. A系统执行事务中属于自己的部分，执行成功之后，在数据库的消息表里记录B系统要执行任务的相关消息，成功后事务完成
2. A系统通过后台的一个定时任务不断的去扫描消息表，将未执行的消息发送到MQ
3. B系统从MQ拉取消息，将要执行的任务插入自己本地的消息表中，然后执行任务，成功之后提交事务，如果失败，自己本地消息表中的数据也会回滚，如果执行成功，则通过zk通知A系统，A系统将自己本地消息表对应的记录状态改成已执行
4. 当B系统某次事务执行失败时，A系统会不断扫描本地消息表，往MQ发送消息，然后B系统不断重试，直到最终成功
5. 当B系统执行成功后，会通过zk通知A系统修改自己的本地事务表，将状态变更为已执行，之后A系统就不会再发送该条消息，分布式事务完成
6. 当B系统执行成功后，通知了A系统，但A系统还未来得及修改本地消息表，又往MQ发送一条消息，由于B系统在执行之前，会先插入本地消息表，因此只要确定一个唯一键，就可以保证幂等性。

> 这种方案是ebay对Base理论的最佳实现，是一种最终一致性思想。
>
> 系统的核心在于A系统在消息未确认之前不断发送MQ，B系统不断重复消费直到成功，MQ有可能发送重复消息，B系统需要保证幂等性，B系统成功之后通知A系统修改消息状态，可以通过zk，也可以通过调用A系统接口实现。
>
> 此方案存在的问题是严重依赖本地消息表，在高并发的场景下可能吞吐量不足。

### <a id="kkxxzzyzx">可靠消息最终一致性</a>

可靠消息最终一致性方案主要是对本地消息表方案的改良，去除了写本地消息表操作，将B系统执行失败由A系统重发消息改由MQ实现，A系统只要向MQ发送一条消息即可。这种方案需要MQ支持事务型消息，例如RocketMQ。整体流程如下：

![image-20201031105541334](http://rocks526.top/lzx/image-20201031105541334.png)

1. A系统向MQ发送pre消息，然后本地执行事务，如果执行成功，发送向MQ发送confirm消息，如果执行失败，将之前的pre消息撤回。
2. 如果A系统执行成功，但发送confirm消息失败，MQ会定时扫描所有的pre消息，回调A系统的接口，确定事务是否执行成功，A系统会再次发送一个confirm或者撤回pre消息，如此即可确保A系统完成事务的话，MQ一定会收到一条消息
3. 当A系统发送的消息confirm之后，B系统就可以消费这条消息，B系统开始执行自己本地事务，如果执行成功，向MQ提交提交这条消息，分布式事务完成
4. 如果B系统执行失败，不会提交这条消息，可以进行反复消费，反复重试直到成功。
5. 和本地消息表一样，MQ可能重复消费，B系统需要保证幂等性，之前是通过本地消息表唯一约束实现的，现在可以通过Redis或者zk实现

> 这种方式是对本地消息表的一种改良，去除了本地消息表的依赖，消息的重发依靠MQ自身实现。
>
> 在这种方案中，B系统如果一直执行失败，也可以不再执行，让B系统本地回滚，然后通过zk或其他方式通知A系统，让A系统也进行回滚，或者可以报警申请人工干预，由人工手动回滚补偿。
>
> 目前采用这种方案的比较多。

### <a id="zdnltzfa">最大努力通知方案</a>

可靠消息最终一致性方案需要依赖MQ实现事务型消息，可能有的MQ不支持，因此又引入下面这种方案，由一个单独的服务从MQ拉取消息，实现消息的重发，整体流程如下：

![image-20201031112345692](http://rocks526.top/lzx/image-20201031112345692.png)

1. 系统A执行本地事务，成功后发送消息到MQ
2. 一个专门的通知服务不断从MQ拉取消息，将消息存储到db或者内存中，然后调用B系统的接口执行任务，如果执行成功，则分布式事务完成，如果失败，则由通知服务定时重试调用B系统接口，如果一直失败，最后可以放弃事务或者人工干预

> 这个方案删除了对MQ事务型消息的依赖，由一个单独的通知服务实现消息的重试

